}
for(p in c(cran.packages, 'reprtree')) eval(substitute(library(pkg), list(pkg=p)))
library(reprtree)
reprtree:::plot.getTree(rf_model)
reprtree:::plot.reprtree(rf_model)
reprtree:::plot.getTree(rf_model)
plot(rf_model)
plot(rf_model)
ggplot(rf_model)
plot(rf_model)
plot(rf_model)
par(mar = rep(2, 4))
plot(rf_model)
plot(rf_model)
graphics.off()
plot(rf_model)
qplot(rf_model)
plot(rf_model)
rf_model = randomForest(r_tree_formula,data = train_set , ntree=600 )
confusionMatrix( predict(rf_model,newdata = cv_set) ,reference = cv_set$class)
library(neuralnet)
nn = neuralnet(formula = r_tree_formula, data= test_set ,hidden = c(5,5,5),linear.output = T  );
r_tree_formula
nn = neuralnet(formula = r_tree_formula, data= test_set ,hidden = c(5,5,5),linear.output = T  );
n_train= model.matrix(r_tree_formula,data =train_set)
n_train
features_formula
n_train= model.matrix( c( "~class" ,features_formula ) ,data =train_set)
n_train= model.matrix( paste("~class",features_formula,sep="+" ) ,data =train_set)
n_train= model.matrix( paste("\~class",features_formula,sep="+" ) ,data =train_set)
n_train= model.matrix( paste("class",features_formula,sep="+" ) ,data =train_set)
featue_names
features_formula
n_train= model.matrix( ~ class + specific_gravity+hemoglobin+serum_creatinine+packed_cell_volume+albumin+red_blood_cell_count+diabetes_mellitus+hypertension+blood_glucose_random ,data =train_set)
n_train
nn = neuralnet(formula = r_tree_formula, data= n_train ,hidden = c(5,5,5),linear.output = T  );
str(train_set)
n_train = train_set
n_train$hypertension = as.integer(n_train$hypertension)
n_train$diabetes_mellitus = as.integer(n_train$diabetes_mellitus)
n_train
n_train$class = as.integer(n_train$class)
str(n_train)
nn = neuralnet(formula = r_tree_formula, data= n_train ,hidden = c(5,5,5),linear.output = T  );
confusionMatrix( predict(nn,newdata = cv_set) ,reference = cv_set$class)
pred = compute(nn ,cv_set$class)
pred = compute(nn , as.integer( cv_set$class)  )
pred = compute(nn , n_train$class )
pred = compute(nn , n_train[,1:9  ] )
pred$net.result
confusionMatrix( pred ,reference = as.integer( cv_set$class) )
confusionMatrix( data = as.factor( pred) ,reference = as.factor( cv_set$class) )
confusionMatrix( data = factor( pred) ,reference = factor( cv_set$class),positive='1' )
pred
nn = neuralnet(formula = r_tree_formula, data= n_train ,hidden = c(5,5,5),linear.output = T  );
pred = compute(nn, cv_set [,1:9 ])
n_cv_set =cv_set
n_cv_set$class = as.numeric( n_cv_set)
n_cv_set$class = as.integer( n_cv_set)
n_cv_set$class = as.integer( n_cv_set$class)
str( n_cv_set)
n_cv_set$diabetes_mellitus = as.integer(n_cv_set$diabetes_mellitus)
n_cv_set$hypertension = as.integer(n_cv_set$hypertension)
pred = compute(nn, cv_set [,1:9 ])
pred = compute(nn, n_cv_set [,1:9 ])
head(pred$net.result)
table(n_cv_set ,   pred$net.result)
library(nnet)
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= c(5,5,5) );
nn_model = nnet(formula =r_tree_formula , data = train_set ,size=10);
nn = neuralnet(formula = r_tree_formula, data= train_set ,hidden = c(5,5,5),linear.output = F  );
plot(nn_model)
confusionMatrix( predict(nn_model,newdata = cv_set) ,reference = cv_set$class)
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn = neuralnet(formula = r_tree_formula, data= n_train ,hidden = c(5,5,5),linear.output = F  );
c_m = compute(nn , n_cv_set[,1:9])
confusionMatrix(c_m ,n_cv_set[,9])
confusionMatrix(c_m ,n_cv_set[,10])
head(c_m)
str(c_m)
confusionMatrix(c_m$net.result ,n_cv_set[,10])
str(c_m$net.result)
ff = as.factor(c_m$net.result)
ff
head(c_m$net.result)
summary(c_m$net.result)
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 5 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 15 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 2 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 2 );
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 2 );
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 2 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 3 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 2 );
yhat <- predict(nn_model, test_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), test_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 2 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), test_set[,10])
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 5 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 10 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 25 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 5 );
yhat <- predict(nn_model, cv_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,10])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 5 );
yhat <- predict(nn_model, test_set[,1:9], type = 'class')
confusionMatrix(as.factor(yhat), test_set[,10])
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(nn_model)
plot.nnet(nn_model, bias=F.)
plot.nnet(nn_model, bias=F)
plot.nnet(nn_model,wts.only = T)
plot.nnet(nn_model,wts.only = T)
plot.nnet(nn_model)
yhat <- predict(nn_model, test_set[,1:ncol(test_set-1) ], type = 'class')
yhat <- predict(nn_model, test_set[,1:ncol(test_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(test_set)])
confusionMatrix(as.factor(yhat), test_set[,ncol(test_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =1 )
yhat <- predict(svm_model, csv_set[,1:ncol(csv)-1 ], type = 'class')
yhat <- predict(svm_model, csv_set[,1:ncol(csv_set)-1 ], type = 'class')
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=500 ,gamma =1 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=500 ,gamma =20 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=200 ,gamma =10 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=200 ,gamma =5 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=200 ,gamma =0.5 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=200 ,gamma =10 )
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =10 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =5 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =1 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =0.5 )
yhat <- predict(svm_model, cv_set[,1:ncol(cv_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), cv_set[,ncol(cv_set)])
yhat <- predict(svm_model, test_set[,1:ncol(test_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), test_set[,ncol(test_set)])
plot(svm_model)
plot(svm_model$kernel)
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =0.5 )
yhat <- predict(svm_model, test_set[,1:ncol(test_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), test_set[,ncol(test_set)])
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =1.5 )
yhat <- predict(svm_model, test_set[,1:ncol(test_set)-1 ], type = 'class')
confusionMatrix(as.factor(yhat), test_set[,ncol(test_set)])
nn_model = nnet(formula =r_tree_formula , data = train_set ,size= 5 );
library("randomForest");
library("e1071");
library(caret);
library(nnet);
### load other R files
source("read_data.R");
source("clean_data.R");
source("features_selection.R")
svm_model = svm(r_tree_formula, data = train_set , cost=100 ,gamma =1.5 )
summarize(raw_data_csv)
summary(raw_data_csv)
str(raw_data_csv)
names(test_set)
d4 .* d5
d4 *. d5
d4 . d5
d4 *. d5
a
library("randomForest");
library("e1071");
library(caret);
library(nnet);
### load other R files
source("read_data.R");
source("clean_data.R");
source("features_selection.R")
############ reading the data #####################
raw_data_csv = read_my_data("file.csv");
hist( raw_data_csv$potassium)
source("clean_data.R");
source("read_data.R");
raw_data_csv = read_my_data("file.csv");
head(raw_data_csv$potassium)
head(raw_data_csv$sodium)
raw_data_csv = read_my_data("file.csv");
################## exploring the data ##################
str(raw_data_csv);
summary(raw_data_csv);
###################clean and normalize the data #####################
cleaned_data =   clean_potassuim_out( clean_sodium_out(  clean_missing(raw_data_csv) ) );
normalized_data = clean_normalization_min_max(cleaned_data);
raw_data_csv = read_my_data("file.csv");
################## exploring the data ##################
str(raw_data_csv);
summary(raw_data_csv);
###################clean and normalize the data #####################
cleaned_data =   clean_potassuim_out( clean_sodium_out(  clean_missing(raw_data_csv) ) );
head(cleaned_data$sodium)
head(cleaned_data$sodium,20)
head(cleaned_data$sodium)
normalized_data = clean_normalization_min_max(cleaned_data);
head(normalized_data$sodium)
staight_data = read.csv("straight_forward_results.csv")
branch_free_data = read.csv("branch_free.csv")
micro_optimized = read.csv("micro_opt_results.csv")
library(ggplot2)
results =read.csv("results_file.csv")
summary(results)
results =read.csv("results_file.csv",header = False)
results =read.csv("results_file.csv",header = F)
summary(results)
names(results)= c("sf","time","type")
summary(results)
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type) +
geom_line() +
geom_point())
results$type = factor(results$type)
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type) +
geom_line() +
geom_point())
results$type
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type) +
geom_line() )
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type) )
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type)  )+
geom_line() +
geom_point()
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type)  )+
geom_line() +
geom_point()+xlab("selectivity factor")+ylab("time in seconds")+ggtitle("performance of branch free predicate vs micro optimized predicate")
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type)  )+
geom_line() +
geom_point()+xlab("selectivity factor")+ylab("time in seconds")+ggtitle("performance of branch free  vs micro optimized predicate")
ggplot(data=results, aes(x= sf  , y=time, group=type, colour=type)  )+
geom_line() +
geom_point()+xlab("selectivity factor")+ylab("time in seconds")+ggtitle("performance of branch free vs micro optimized predicate")
results =read.csv("results_file6.csv",header = F)
names(results)= c("sf","time","type")
source("draw_results.R")
source("draw_results.R")
my_plot
source("draw_results.R")
install.packages("MonetDB.R")
library(MonetDB.R)
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="thcp2", user="monetdb", password="monetdb")
install.packages("RMySQL")
library("RMySQL")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="thcp2", user="monetdb", password="monetdb")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="thcp2", user="monetdb", password="monetdb")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="thcp2", user="monetdb", password="monetdb")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="thcp2", user="monetdb", password="monetdb")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="thcp2", user="monetdb", password="monetdb")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="tpch2", user="monetdb", password="monetdb")
conn <- dbConnect(MonetDB.R(), host="localhost", dbname="tpch2", user="monetdb", password="monetdb")
dbGetQuery(conn,"SELECT 'hello world'")
dbGetQuery(conn,"SELECT count(*) from 'lineitem' ")
dbGetQuery(conn,"SELECT count(*) from lineitem")
dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_shipdate'; ")
a = dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_shipdate'; ")
a
a + 1
l_shipdate = dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_shipdate'; ")
l_shipdate_size = dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_shipdate'; ")
l_discount_size = dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_discount'; ")
l_quantity = dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_quantity'; ")
l_extendedprice = dbGetQuery(conn,"select  count * typewidth   from  sys.storage() where table = 'lineitem' and column ='l_extendedprice'; ")
l_shipdate = NULL
total_data_processed = l_shipdate_size + l_quantity + l_extendedprice + l_discount_size
total_data_processed
q6 = dbGetQuery(conn,"<\ /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql ")
q6 = dbGetQuery(conn,"<\ /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql ");
q6 = dbGetQuery(conn,"<\ '/home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql' ");
q6 = dbGetQuery(conn,"<\ '/home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql'; ");
q6 = dbGetQuery(conn,"'<\' /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql; ");
q6 = dbGetQuery(conn,"\< /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql; ");
q6 = dbGetQuery(conn," \< /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql; ");
q6 = dbGetQuery(conn," '\<' /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/1-load_data.SF-1.sql; ");
q6 = dbGetQuery(conn," \< /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/q06.sql ");
q6 = dbGetQuery(conn," '\<' /home/pegasus/Schreibtisch/current task/tpch_2_17_1/dbgen/MonetDB/q06.sql ");
q6= "select
sum(l_extendedprice * l_discount) as revenue
from
lineitem
where
l_shipdate >= date '1994-01-01'
and l_shipdate < date '1994-01-01' + interval '1' year
and l_discount between .06 - 0.01 and .06 + 0.01
and l_quantity < 24; ";
dbGetQuery(conn,q6);
dbGetQuery(conn," delete * from lineitem ");
dbGetQuery(conn," delete * from 'lineitem' ");
dbGetQuery(conn," delete  from 'lineitem' ");
dbGetQuery(conn," delete  from lineitem ");
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="\|");
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|");
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|", overwrite=TRUE);
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|", append=TRUE);
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|", overwrite=TRUE);
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|);
""
"
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|");
monetdb.read.csv(conn, "lineitem.tbl",append=T  ,"lineitem",delim="|");
monetdb.read.csv(conn, "lineitem.tbl", "lineitem",delim="|");
create_table_q ="CREATE TABLE lineitem (
l_orderkey	INT	NOT NULL,
l_partkey	INT	NOT NULL,
l_suppkey	INT	NOT NULL,
l_linenumber	INT	NOT NULL,
l_quantity	DECIMAL(15,2)	NOT NULL,
l_extendedprice	DECIMAL(15,2)	NOT NULL,
l_discount	DECIMAL(15,2)	NOT NULL,
l_tax	DECIMAL(15,2)	NOT NULL,
l_returnflag	CHAR(1)	NOT NULL,
l_linestatus	CHAR(1)	NOT NULL,
l_shipdate	DATE	NOT NULL,
l_commitdate	DATE	NOT NULL,
l_receiptdate	DATE	NOT NULL,
l_shipinstruct	CHAR(25)	NOT NULL,
l_shipmode	CHAR(10)	NOT NULL,
l_comment	VARCHAR(44)	NOT NULL
);"
dbGetQuery(conn,create_table_q);
dbWriteTable(conn,  "lineitem",read.csv("lineitem",sep="|",header="F"),overwrite=TRUE)
dbWriteTable(conn,  "lineitem",read.csv(file = "lineitem.tbl",sep="|",header=F),overwrite=TRUE)
dbGetQuery(conn,create_table_q);
dbWriteTable(conn,  "lineitem",read.csv(file = "lineitem.tbl",sep="|",header=F),append=TRUE)
f = read.csv(file = "lineitem.tbl",sep="|",header=F),append=TRUE)
f = read.csv(file = "lineitem.tbl",sep="|",header=F))
f = read.csv(file = "lineitem.tbl",sep="|",header=F)
head(f)
f = read.csv(file = "lineitem.tbl",sep=c("|","\n"),header=F)
f = read.csv(file = "region.tbl",sep=c("|","\n"),header=F)
head(f)
f = read.csv(file = "region.tbl",sep=c("|"," "),header=F)
head(f)
f = read.csv(file = "region.tbl",sep=c("|\n"),header=F)
f = read.csv(file = "region.tbl",sep="|\n",header=F)
f = read.csv(file = "region.tbl",sep="|")
head(f)
f = read.csv(file = "region.tbl",sep=c("|"," ","\n") )
f = read.csv(file = "region.tbl",sep=c("|"," ","\n"),header = F )
f = read.csv(file = "region.tbl",sep=("|","\n")  )
f = read.csv(file = "region.tbl",sep=("|"), blank.lines.skip=T )
f = read.csv(file = "region.tbl",sep=("|"), strip.white=T )
f = read.csv(file = "region.tbl",sep=("|"), skipNul=T )
head(f)
f = read.csv(file = "region.tbl",sep=("|"), allowEscapes = T )
f = read.csv(file = "lineitem.tbl",sep="|",header=F)
head(f[;1:ncol(f)  ])
head() f[ncol(F)]
head(f[ncol(f)])
f[ncol(f)] = NULL
dbWriteTable(conn,  "lineitem",f,append=TRUE)
f=NULL
gc()
save.image("~/vvv.RData")
f = read.csv(file = "lineitem.tbl", header = F )
f = NULL
gc()
setwd("~/Schreibtisch/Untitled Folder/plots/exp1/workingset doesn't fit")
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1]*(60),inno_csv$V3[2]*(60),inno_csv$V3[3]*(60),inno_csv$V3[4]*(60),inno_csv$V3[5]*(60),inno_csv$V3[6]*(60),inno_csv$V3[7]*(60),memory_csv$V3*(60))
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput as QPH') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1]*(60),inno_csv$V3[2]*(60),inno_csv$V3[3]*(60),inno_csv$V3[4]*(60),inno_csv$V3[5]*(60),inno_csv$V3[6]*(60),inno_csv$V3[7]*(60),memory_csv$V3*(60))
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput as QPH') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1],inno_csv$V3[2],inno_csv$V3[3],inno_csv$V3[4],inno_csv$V3[5],inno_csv$V3[6],inno_csv$V3[7],memory_csv$V3)
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput as QPH') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1]*(60),inno_csv$V3[2]*(10),inno_csv$V3[3]*(10),inno_csv$V3[4]*(10),inno_csv$V3[5]*(10),inno_csv$V3[6]*(10),inno_csv$V3[7]*(10),memory_csv$V3*(10))
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput as QPH') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1]*(60),inno_csv$V3[2]*(60),inno_csv$V3[3]*(60),inno_csv$V3[4]*(60),inno_csv$V3[5]*(60),inno_csv$V3[6]*(60),inno_csv$V3[7]*(60),memory_csv$V3*(60))
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput as QPH') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1]*(60),inno_csv$V3[2]*(60),inno_csv$V3[3]*(60),inno_csv$V3[4]*(60),inno_csv$V3[5]*(60),inno_csv$V3[6]*(60),inno_csv$V3[7]*(60),memory_csv$V3*(60))
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput as QP60H') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
library(plotly)
memory_csv=read.csv(file = "mem.csv", header =  F);
inno_csv=read.csv(file = "inno.csv", header =  F);
configurations <- c("bfr=0.15", "bfr=0.30", "bfr=0.45","bfr=0.60","bfr=0.75","bfr=0.90","bfr=1.0","memory")
latency <- c(inno_csv$V2[1]/(60*60),inno_csv$V2[2]/(60*60),inno_csv$V2[3]/(60*60),inno_csv$V2[4]/(60*60),inno_csv$V2[5]/(60*60),inno_csv$V2[6]/(60*60),inno_csv$V2[7]/(60*60),memory_csv$V2/(60*60))
throughput <- c(inno_csv$V3[1]*(60),inno_csv$V3[2]*(60),inno_csv$V3[3]*(60),inno_csv$V3[4]*(60),inno_csv$V3[5]*(60),inno_csv$V3[6]*(60),inno_csv$V3[7]*(60),memory_csv$V3*(60))
data <- data.frame(configurations, latency, throughput);
m <- list(
l = 50,
r = 200,
b = 100,
t = 0,
pad = 1
)
plot_ly(data, x = ~configurations, y = ~latency, type = 'bar', name = 'Latency in Hours') %>%
add_trace(y = ~throughput, name = 'throughput QP60H') %>%
layout(autosize = F, width = 750, height = 350,margin=m,yaxis = list(title = 'Count'), barmode = 'group')
